{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is the implementation of the optimiazed search engine at user's end.\n",
    "#### 1. Search for an optimal product based on the reviews using similarity matrix, positive sentiment, and weights of the keywords selected by the user.\n",
    "#### 2. Retrieve the products with the reviews based on its relevancy to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U git+https://github.com/huggingface/transformers.git\n",
    "# !pip install rake-nltk    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jungakim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jungakim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jungakim/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jungakim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from tensorflow import keras\n",
    "\n",
    "import spacy\n",
    "# !python -m spacy download en\n",
    "from itertools import chain\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "from gensim.summarization import keywords as keywords_extractor\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from rake_nltk import Rake\n",
    "\n",
    "import gensim.downloader as api\n",
    "from utils import * \n",
    "# Contains computing positive similarity score(get_similarity_score_with_product), \n",
    "# get_sentiment_scores, similarity_scores, sentimental_similarity_score_of_a_review, get_similarity_score_with_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset with 10 or more reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_pickle('data/products_10_or_more_reviews.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:resolver HttpCompressedFileResolver does not support the provided handle.\n",
      "INFO:absl:resolver GcsCompressedFileResolver does not support the provided handle.\n"
     ]
    }
   ],
   "source": [
    "embed = hub.load('use')\n",
    "# embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\") # Can load from Tensorflow_hub link\n",
    "model = keras.models.load_model('model') # From google colab. 40 epochs 2**16 batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_reddit = AutoTokenizer.from_pretrained(\"google/pegasus-reddit_tifu\")\n",
    "model_reddit = AutoModelForSeq2SeqLM.from_pretrained(\"google/pegasus-reddit_tifu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings of the product titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_embeddings = pd.read_pickle('data/product_embeds.pkl') # Run google colab GPU to get the embeddings of all unique products in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the embeddings of the product titles<br>\n",
    "Done in Google Colab with GPU.\n",
    "<code style=\"font-size: 10px; background-color:transparent;\">\n",
    "def tensors_from_series(series, lower, upper):\n",
    "  return tf.convert_to_tensor(series.iloc[lower:upper].apply(lambda x: str(x)).values)\n",
    "unique_products = reviews[['product_parent','product_title']].drop_duplicates()\n",
    "product_names = unique_products.product_title\n",
    "chunksize = unique_products.shape[0] // 1000\n",
    "bounds = chunksize * np.arange(unique_products.shape[0] // chunksize)\n",
    "<code style=\"font-size: 10px; background-color:transparent;\">\n",
    "bounds_tuples = list()\n",
    "for i, bound in enumerate(bounds):\n",
    "  if i == unique_products.shape[0] // chunksize - 1: break\n",
    "  bounds_tuples.append((bounds[i], bounds[i+1]))\n",
    "bounds_tuples.append((bounds_tuples[-1][1], unique_products.shape[0]))\n",
    "<br>\n",
    "with tf.device('/GPU:0'):\n",
    "  print(f\"{len(bounds_tuples)} chunks to embed.\")\n",
    "  embeddings = embed(tensors_from_series(product_names, 0, bounds_tuples[0][1]))\n",
    "  for i, bounds_tuple in enumerate(bounds_tuples[1:]):\n",
    "    if i % 100 == 0: \n",
    "      print(i,end='  ')\n",
    "    embeddings = tf.concat([embeddings, embed(tensors_from_series(product_names, bounds_tuple[0], bounds_tuple[1]))], 0)\n",
    "product_embeddings = embeddings.numpy()\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for optimal products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Within the categorily matched top_k_products, look at the top_k helpful reviews and save their \"sentiment * similarity\" scores.**\n",
    "<br><br>\n",
    "Positive similarity score of a sentence(in a review) is defined here as<br>\n",
    "&emsp;\"its predicted sentiment probability * (weighted) similarity score with a keyword/query sentence\"\n",
    "<br><br>\n",
    "Positive similarity score of a review is the average of the maximum positive similarity score of a sentence of all keywords/query sentences.<br>\n",
    "Following pseudo code using for-loop or the vectorization with matrices may better explain this concept.<br>\n",
    "\n",
    "**For-loop** to compute the positive similarity score between a review and a query\n",
    "<code style=\"font-size: 11px; background-color:transparent;\">\n",
    "For each \"sentence in a review\":\n",
    "    Compute the positive similarity with each \"keyword of a query\" using 'get_sentiment_scores' and 'similarity_scores' functions.\n",
    "    Get the maximum positive similarity out of all keywords(positive similarity = sentiment score * similarity score).\n",
    "Repeat the same for-loop for \"sentences in a query\".\n",
    "pos_sim_score_keywords = Mean of the maximum positive similarities.\n",
    "pos_sim_score_sentences = Mean of the maximum positive similarities.\n",
    "Return the smaller(being conservative) one from pos_sim_score_keywords and pos_sim_score_sentences.\n",
    "</code><br>\n",
    "**Vectorization** to compute the positive similarity score between a review and a query<br><br>\n",
    "$\\begin{bmatrix} & \\scriptsize\\textit{query's Keyword1} & \\scriptsize\\textit{query's Keyword2} & \\scriptsize\\textit{query's Keyword3} &  \\\\ \n",
    "\\scriptsize\\textit{Review's Sentence1} & \\scriptsize\\text{Similarity} & \\scriptsize\\text{Similarity} & \\scriptsize\\text{Similarity} & \\cdots \\\\ \n",
    "\\scriptsize\\textit{Review's Sentence2} & \\scriptsize\\text{Similarity} & \\scriptsize\\text{Similarity} & \\scriptsize\\text{Similarity} & \\cdots \\\\ \n",
    "\\scriptsize\\textit{Review's Sentence3} & \\scriptsize\\text{Similarity} & \\scriptsize\\text{Similarity} & \\scriptsize\\text{Similarity} & \\cdots \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\end{bmatrix}$\n",
    "*\n",
    "$\\begin{bmatrix} \\scriptsize\\text{Sentiment score of Review's Sentence1} \\\\ \n",
    "\\scriptsize\\text{Sentiment score of Review's Sentence2} \\\\ \\scriptsize\\text{Sentiment score of Review's Sentence3} \\\\ \\vdots \\end{bmatrix}$\n",
    "*\n",
    "$\\begin{bmatrix} \\scriptsize\\text{Weight of query's Keyword1} & \\scriptsize\\text{Weight of query's Keyword2} & \\cdots \\end{bmatrix}$\n",
    "$=\\begin{bmatrix}\n",
    "\\scriptsize\\textit{Review's Sentence1} & \\scriptsize\\text{similarity with Keyword1 * sentiment * weight} & \\scriptsize\\text{Keyword2's similarity * sentiment * weight} & \\cdots \\\\ \n",
    "\\scriptsize\\textit{Review's Sentence2} & \\scriptsize\\text{similarity with Keyword2 * sentiment * weight} & \\scriptsize\\text{Keyword2's similarity * sentiment * weight} & \\cdots \\\\ \n",
    "\\scriptsize\\textit{Review's Sentence3} & \\scriptsize\\text{similarity with Keyword3 * sentiment * weight} & \\scriptsize\\text{Keyword2's similarity * sentiment * weight} & \\cdots \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\end{bmatrix}$\n",
    "$\\leftarrow$ Take the max (axis=1) $\\rightarrow$ shape = (num_sentences, 1) $\\rightarrow$ Take the mean(axis = 0) $\\rightarrow$ scalar $\\Leftarrow$ This measures the positivity and similarity of a review with respect to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Type the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"backlit wireless keyboard\" #  <---------- Set by User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the products of most likely category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.7 s, sys: 5.32 s, total: 42.1 s\n",
      "Wall time: 26.6 s\n"
     ]
    }
   ],
   "source": [
    "%time most_similar_indices, sim_scores = get_similarity_score_with_product(query, euclidean=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set up the hyperparameters\n",
    "- top_k_products: Choose the number of products to search (from 'most_similar_indices')\n",
    "- top_k_reviews: Choose the number of reviews to search (sorted by 'helpful_votes' column)\n",
    "- emphasized_keywords: list of keywords that user will define to be more emphasized(important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_products, top_k_reviews, emphasized_keywords = 5, 5, ['wireless', 'keyboard'] #  <---------- Set by User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search the products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will examine reviews of 5 products that are most similar to the query\n",
      "Processing the review of product\n",
      "...1...2...3...4...5"
     ]
    }
   ],
   "source": [
    "print(f\"We will examine reviews of {top_k_products} products that are most similar to the query\")\n",
    "matching_scores = defaultdict(pd.Series)\n",
    "unique_products = reviews[['product_parent','product_title']].drop_duplicates()\n",
    "matching_products = unique_products.iloc[most_similar_indices[:top_k_products],0].values\n",
    "\n",
    "print(\"Processing the review of product\")\n",
    "for i, product_id in enumerate(matching_products):\n",
    "    print(f\"...{i+1}\", end='')\n",
    "    reviews_list = reviews.loc[reviews.product_parent == product_id, ['review_body', 'helpful_votes', 'review_date']].\\\n",
    "    sort_values(ascending = False, by = ['helpful_votes', 'review_date'])[:top_k_reviews]['review_body']\n",
    "    matching_scores[product_id] = reviews_list.apply(sentimental_similarity_score_of_a_review, args=(query, 'positive', emphasized_keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print the product with the highest mean sentiment * similarity score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your query matched with:\n",
      "RK728 Wireless Keyboard\n",
      "\n",
      "The median of the Geometric mean of Positive Similarity score between \"backlit wireless keyboard\" and \"RK728 Wireless Keyboard\" is 0.279\n"
     ]
    }
   ],
   "source": [
    "index = 0 #  <---------- Set by User\n",
    "\n",
    "med_matching_scores = [np.nanmedian(v) for k, v in matching_scores.items()]\n",
    "product_scores = np.sort(med_matching_scores)[::-1]\n",
    "product_indices = np.argsort(med_matching_scores)[::-1]\n",
    "matched_product_id = matching_products[product_indices[index]]\n",
    "product_title = reviews.loc[reviews.product_parent == matched_product_id, :].head(1).product_title.values[0]\n",
    "print(f\"Your query matched with:\\n{product_title}\")\n",
    "print(f\"\\nThe median of the Geometric mean of Positive Similarity score between \\\"{query}\\\" and \\\"{product_title}\\\" is {product_scores[index]:.3f}\")\n",
    "# Geometric mean takes into account the effect of compounding, therefore, better suited for calculating the returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the matched product, create a dataframe with whether the ratings are 5 or not and their similarity scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_reviews = reviews.loc[reviews.product_parent == matched_product_id, ['review_body', 'star_rating', 'review_date']]\n",
    "matched_reviews['review_date'] = matched_reviews['review_date'].apply(lambda x: str(x).split()[0])\n",
    "\n",
    "pos = (matched_reviews.star_rating == 5.0).astype(int)\n",
    "sim = matched_reviews.review_body.apply(lambda x: sentimental_similarity_score_of_a_review(x, query, None, emphasized_keywords) if x is not None else None)\n",
    "pos_sim = pd.concat([pos.rename('positive'), sim.rename('similarity')], axis=1)\n",
    "matched_reviews[['positive', 'similarity']] = pos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create summary if the review is too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary(text):\n",
    "    \"\"\"\n",
    "    Create summary of the input text using the NLP model\n",
    "    \"\"\"\n",
    "    tokenized_text = tokenizer_reddit.encode(text, return_tensors=\"pt\", truncation=True)\n",
    "    summary_ids = model_reddit.generate(tokenized_text,\n",
    "                                          num_beams=4,\n",
    "                                          no_repeat_ngram_size=2,\n",
    "                                          min_length=30,\n",
    "                                          max_length=150,#  <---------- Set by User\n",
    "                                          early_stopping=True)\n",
    "    return \"...\"+tokenizer_reddit.decode(summary_ids[0], skip_special_tokens=True)+\"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select 'show_k' number of reviews for positive rating(5.0) and negative rating(<=4.0) which are ordered in most similar-to-query manner.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_k = 5 #  <---------- Set by User\n",
    "maximum_len = 300 # Maximum length not to be summarized  <---------- Set by User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sim_indices = pos_sim.loc[pos_sim.positive == 1, 'similarity'].sort_values(ascending = False).head(show_k).index\n",
    "neg_sim_indices = pos_sim.loc[pos_sim.positive == 0, 'similarity'].sort_values(ascending = False).head(show_k).index\n",
    "\n",
    "k_pros = matched_reviews.loc[pos_sim_indices, ['review_body', 'star_rating', 'review_date', 'similarity']]\n",
    "k_cons = matched_reviews.loc[neg_sim_indices, ['review_body', 'star_rating', 'review_date', 'similarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_a02af116_f6f4_11ea_bb7e_f45c899083edrow0_col0,#T_a02af116_f6f4_11ea_bb7e_f45c899083edrow1_col0,#T_a02af116_f6f4_11ea_bb7e_f45c899083edrow2_col0,#T_a02af116_f6f4_11ea_bb7e_f45c899083edrow3_col0,#T_a02af116_f6f4_11ea_bb7e_f45c899083edrow4_col0{\n",
       "            width:  600px;\n",
       "        }</style><table id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083ed\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >review_body</th>        <th class=\"col_heading level0 col1\" >star_rating</th>        <th class=\"col_heading level0 col2\" >review_date</th>        <th class=\"col_heading level0 col3\" >similarity</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083edlevel0_row0\" class=\"row_heading level0 row0\" >2903224</th>\n",
       "                        <td id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083edrow0_col0\" class=\"data row0 col0\" >I used this with my laptop because I never liked the feel of the crammed laptop layout.     It allows me to comfortably type in crammed spaces while in flight. Plus I like to type on my lap and is so much better not having a hot heavy notebook on my lap.</td>\n",
       "                        <td id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083edrow0_col1\" class=\"data row0 col1\" >5.000000</td>\n",
       "                        <td id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083edrow0_col2\" class=\"data row0 col2\" >2008-07-18</td>\n",
       "                        <td id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083edrow0_col3\" class=\"data row0 col3\" >0.58</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083edlevel0_row1\" class=\"row_heading level0 row1\" >2900544</th>\n",
       "                        <td id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083edrow1_col0\" class=\"data row1 col0\" >If you are looking for the the keyboard to complete your laptop/hdtv combination THIS IS PERFECT. I just got this last week opened the box plugged it in and it started surfing the net rite away never even rebooted the computer. Netflicks has never been better!!!!</td>\n",
       "                        <td id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083edrow1_col1\" class=\"data row1 col1\" >5.000000</td>\n",
       "                        <td id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083edrow1_col2\" class=\"data row1 col2\" >2008-08-02</td>\n",
       "                        <td id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083edrow1_col3\" class=\"data row1 col3\" >0.58</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083edlevel0_row2\" class=\"row_heading level0 row2\" >2903353</th>\n",
       "                        <td id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083edrow2_col0\" class=\"data row2 col0\" >this product is similar to the FK760 I purchased recently and it has an integrated touchpad.  I like it.  Very good price and the company delivered the product quickly.</td>\n",
       "                        <td id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083edrow2_col1\" class=\"data row2 col1\" >5.000000</td>\n",
       "                        <td id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083edrow2_col2\" class=\"data row2 col2\" >2008-07-17</td>\n",
       "                        <td id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083edrow2_col3\" class=\"data row2 col3\" >0.41</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083edlevel0_row3\" class=\"row_heading level0 row3\" >1258198</th>\n",
       "                        <td id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083edrow3_col0\" class=\"data row3 col0\" >Loved It</td>\n",
       "                        <td id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083edrow3_col1\" class=\"data row3 col1\" >5.000000</td>\n",
       "                        <td id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083edrow3_col2\" class=\"data row3 col2\" >2014-07-22</td>\n",
       "                        <td id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083edrow3_col3\" class=\"data row3 col3\" >0.35</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083edlevel0_row4\" class=\"row_heading level0 row4\" >2117844</th>\n",
       "                        <td id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083edrow4_col0\" class=\"data row4 col0\" >good staff, we like it very much easy to use and easy to operations, no problems at alll, if you like it go get it.</td>\n",
       "                        <td id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083edrow4_col1\" class=\"data row4 col1\" >5.000000</td>\n",
       "                        <td id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083edrow4_col2\" class=\"data row4 col2\" >2013-02-13</td>\n",
       "                        <td id=\"T_a02af116_f6f4_11ea_bb7e_f45c899083edrow4_col3\" class=\"data row4 col3\" >0.06</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fa504ccf690>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = k_pros.review_body.map(lambda x: len(x)) >= maximum_len\n",
    "\n",
    "k_pros.loc[mask, 'original'] = k_pros.loc[mask, 'review_body'].values\n",
    "k_pros.loc[mask, 'review_body'] = k_pros.loc[mask, 'review_body'].apply(create_summary)\n",
    "k_pros.loc[:, 'similarity'] = k_pros.loc[:, 'similarity'].apply(lambda x: str(round(x,2)))\n",
    "k_pros.loc[:, ['review_body', 'star_rating', 'review_date', 'similarity']].style.set_properties(subset=['review_body'], **{'width': '600px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow0_col0,#T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow1_col0,#T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow2_col0,#T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow3_col0,#T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow4_col0{\n",
       "            width:  600px;\n",
       "        }</style><table id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083ed\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >original</th>        <th class=\"col_heading level0 col1\" >review_body</th>        <th class=\"col_heading level0 col2\" >star_rating</th>        <th class=\"col_heading level0 col3\" >review_date</th>        <th class=\"col_heading level0 col4\" >similarity</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edlevel0_row0\" class=\"row_heading level0 row0\" >2847875</th>\n",
       "                        <td id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow0_col0\" class=\"data row0 col0\" >nan</td>\n",
       "                        <td id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow0_col1\" class=\"data row0 col1\" >this keyboard has some very nice features, but it is very hard to push the keys down, and the keyboard is cramped.  also there is some delay on the touch pad, not much, but fine movements are hard.  this is a great keyboard for use with a media center, but not for your primary keyboard.</td>\n",
       "                        <td id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow0_col2\" class=\"data row0 col2\" >3.000000</td>\n",
       "                        <td id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow0_col3\" class=\"data row0 col3\" >2009-03-18</td>\n",
       "                        <td id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow0_col4\" class=\"data row0 col4\" >0.68</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edlevel0_row1\" class=\"row_heading level0 row1\" >2882919</th>\n",
       "                        <td id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow1_col0\" class=\"data row1 col0\" >It's good to have this option, I need a keyboard being able to put on lap to type, my shoulder keeps pain if I use keyboard on high table. This product is light enough. And wireless is also needed, there is no so many choices on the market. USB receiver is good. setup is easy.  A bad is that the key press, they should really making keypress better. Plus the mouse point move not fast using the built-in touchpad.  A good compliment, but sometimes you would prefer changing back to normal keyboard/mouse</td>\n",
       "                        <td id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow1_col1\" class=\"data row1 col1\" >...wireless keyboard/mouse combo is a good compliment, but sometimes you would prefer changing back to normal keyboard or mouse. ;-(;)...</td>\n",
       "                        <td id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow1_col2\" class=\"data row1 col2\" >3.000000</td>\n",
       "                        <td id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow1_col3\" class=\"data row1 col3\" >2008-11-12</td>\n",
       "                        <td id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow1_col4\" class=\"data row1 col4\" >0.58</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edlevel0_row2\" class=\"row_heading level0 row2\" >2829066</th>\n",
       "                        <td id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow2_col0\" class=\"data row2 col0\" >I have a wireless system and the idea of  separate board and mouse on the couch are ridiculous to me so I've tried several of these keyboards with pointing devices built in.  I have to say this is the best one so far.  The Media center buttons are great and probably the strong point.  The cons are the key return and layout are horrible,  if you touch type you will see at least a 50% drop in your speed and accuracy.  The Touch pad it's self is quirky jumping and hanging, I find I have to actually bang the unit to get it to go back to normal,  and you often have to re-sync with the dongle, which is a pain.  But in the end it's cool and it works, maybe I'll get better on the touch typing but it's been several months so I doubt it.</td>\n",
       "                        <td id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow2_col1\" class=\"data row2 col1\" >...don't touch type on a wireless keyboard, it's a pain and you'll get a 50% drop in speed and accuracy if you touch....</td>\n",
       "                        <td id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow2_col2\" class=\"data row2 col2\" >3.000000</td>\n",
       "                        <td id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow2_col3\" class=\"data row2 col3\" >2009-06-10</td>\n",
       "                        <td id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow2_col4\" class=\"data row2 col4\" >0.56</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edlevel0_row3\" class=\"row_heading level0 row3\" >1715141</th>\n",
       "                        <td id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow3_col0\" class=\"data row3 col0\" >nan</td>\n",
       "                        <td id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow3_col1\" class=\"data row3 col1\" >Good as a Media Center keyboard. Trackpad works ok, but it is old style (does not click as Macbook ones do). Decent range.<br />Cons: No backlight</td>\n",
       "                        <td id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow3_col2\" class=\"data row3 col2\" >3.000000</td>\n",
       "                        <td id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow3_col3\" class=\"data row3 col3\" >2013-11-24</td>\n",
       "                        <td id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow3_col4\" class=\"data row3 col4\" >0.55</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edlevel0_row4\" class=\"row_heading level0 row4\" >2480950</th>\n",
       "                        <td id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow4_col0\" class=\"data row4 col0\" >PROS:<br />  Battery life is fantastic.  Range is superb.  Tactile feedback is exactly what you expect from a real keyboard.<br /><br />CONS:<br />  The track-pad can be a bit goofy.  The keyboard doesn't have an indicator for NumLock or CapsLock... this really bugs me. It's just a bit too big for coffee table use.</td>\n",
       "                        <td id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow4_col1\" class=\"data row4 col1\" >...keyboard is too big for coffee table use./b>br /> a href=\"http://i.imgur.com/a/f0ttxx\" target=\"_blank\" rel=\"book\" title=\"https://www.amazon.co.uk/gp/product/listing.asp?ie=u-bloblot&qid=1&sr=0&keywords=keyboard\" width=\"640\" height=\"480\" />...</td>\n",
       "                        <td id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow4_col2\" class=\"data row4 col2\" >3.000000</td>\n",
       "                        <td id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow4_col3\" class=\"data row4 col3\" >2011-12-18</td>\n",
       "                        <td id=\"T_c04bfcec_f6f4_11ea_bb7e_f45c899083edrow4_col4\" class=\"data row4 col4\" >0.49</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fa7948ef0d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = k_cons.review_body.map(lambda x: len(x)) >= maximum_len\n",
    "\n",
    "k_cons.loc[mask, 'original'] = k_cons.loc[mask, 'review_body'].values\n",
    "k_cons.loc[mask, 'review_body'] = k_cons.loc[mask, 'review_body'].apply(create_summary)\n",
    "k_cons.loc[:, 'similarity'] = k_cons.loc[:, 'similarity'].apply(lambda x: str(round(x,2)))\n",
    "k_cons.loc[:, ['original', 'review_body', 'star_rating', 'review_date', 'similarity']].style.set_properties(subset=['original'], **{'width': '600px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it's not used here. Using Rake and NLTK, find the probable category of the query\n",
    "<code style=\"font-size: 11px; background-color:transparent;\">\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "r = Rake()# Uses stopwords for english from NLTK, and all puntuation characters.\n",
    "def get_ranked_phrases(query):\n",
    "    r.extract_keywords_from_text(query)\n",
    "    phrases = {token for token in r.get_ranked_phrases()}\n",
    "    nouns = {word.lower() for word, pos in pos_tag(word_tokenize(query)) if pos in ('NN', 'NNS', 'NNP')}\n",
    "    ranked_phrases = list(chain(*[[token for token in phrases if noun in token.split()] for noun in nouns]))\n",
    "    ranked_phrases_split = [set(query_product.split()) for query_product in ranked_phrases]\n",
    "    category_set = [{lemmatizer.lemmatize(token) for token in token_set} for token_set in ranked_phrases_split]\n",
    "    return category_set\n",
    "</code><br>\n",
    "Although it's not used here.\n",
    "Word Movers' Distance from [link](https://medium.com/@Intellica.AI/comparison-of-different-word-embeddings-on-text-similarity-a-use-case-in-nlp-e83e08469c1c)\n",
    "<code style=\"font-size: 10px; background-color:transparent;\">\n",
    "stop_words = stopwords.words('english')\n",
    "def preprocess(sentence):\n",
    "    return [w for w in sentence.lower().split() if w not in stop_words]\n",
    "model_wv = api.load('word2vec-google-news-300')\n",
    "query = \"quiet keyboards\"\n",
    "product_titles = unique_products.product_title.map(preprocess)\n",
    "%time sim_scores = product_titles.apply(model_wv.wmdistance, args = (preprocess(query), ))\n",
    "unique_products.iloc[np.argsort(sim_scores)[::-1], 1].head(10).values\n",
    "</code>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "use-tf",
   "language": "python",
   "name": "use-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
